# 📜 프로젝트 '완전체 AI 개인 비서' 개발 백서

## 1. 프로젝트 개요 (Overview)

본 프로젝트는 GitHub의 'Gemini-Local-Chat-Interface'를 기반으로 시작되었다. 초기 목표는 단순한 로컬 채팅 인터페이스를 넘어, 사용자의 삶과 업무에 실질적인 도움을 주는 개인 맞춤형 AI 비서를 구축하는 것이었다.

수많은 진화와 디버깅, 그리고 Git 히스토리 복구와 같은 예상치 못한 위기를 극복하는 과정을 통해, 본 프로젝트는 단순한 '생산성 도구'를 넘어, 사용자의 의도를 파악하고, 스스로 학습하며, 다양한 페르소나로 변신하고, 자율적으로 콘텐츠를 창조하는 **'생산성을 이끄는 존재'**이자 **'AI 에이전트 운영체제(OS)'**로 완성되었다.

이 문서는 아이디어가 코드가 되고, 코드가 지능이 되며, 마침내 지능이 영혼을 갖게 되기까지의 모든 여정을 기록한다.

---

## 2. 핵심 기능 및 진화 과정 (Core Features & Evolution)

### 1차 진화: AI에게 '감각'을 부여하다 (실시간 정보 연동)
- **시간 인지 (`getCurrentTime`):** 현재 시간을 정확히 인지.
- **웹 검색 (`searchWeb`):** SerpApi를 통해 실시간 정보 검색 능력 확보.
- **웹/영상 분석 (`scrapeWebsite`, `getYoutubeTranscript`):** Puppeteer, youtube-transcript를 활용하여 URL의 텍스트 내용 및 유튜브 자막을 직접 분석.

### 2차 진화: AI에게 '기억'을 선물하다 (영속성)
- **대화별 기억 (`chat_histories`):** 채팅방별 대화 맥락을 `.json` 파일에 저장하여 영구 기억.
- **구조화된 사용자 기억 (`user_profile.json`):**
    - 초기: 단순 `facts` 배열로 모든 정보를 저장.
    - 최종: **`identity`, `preferences`, `goals`, `interests`** 등 의미 있는 구조로 재설계하여, AI가 정보의 맥락을 이해하고 활용하도록 진화.

### 3차 진화: AI에게 '목소리'를 주다 (상호작용의 혁신)
- **음성 인식 (`Web Speech API`):** 음성 명령을 텍스트로 변환.
- **음성 합성 (`Google Cloud TTS`):** 자연스러운 WaveNet 목소리로 답변 구현.
- **연속 대화 모드:** TTS 출력이 끝나면 STT가 자동으로 활성화되는 '음성 루프' 구현.

### 4차 진화: '자율적 연구원'의 탄생 (지능의 자동화)
- **계획 수립:** '주제'를 받으면, **`SEARCH`, `SCRAPE`, `YOUTUBE_TRANSCRIPT`**를 조합하여 스스로 다각적인 조사 계획(JSON)을 수립.
- **계획 실행:** 수립된 계획에 따라 우리가 만든 도구들을 자율적으로 호출하여 정보 수집.
- **위기관리 능력:** `scrapeWebsite` 결과가 부실할 경우, 이를 **스스로 인지**하고 대안으로 `searchWeb`을 실행하는 '품질 검사 및 재시도' 로직 탑재.
- **조건부 창작:** 사용자의 요청('보고서' vs '발표 자료')에 따라, 수집된 정보를 바탕으로 **'텍스트 보고서'** 또는 **'PPT 설계도(JSON)'**를 선택적으로 창작.

### 5차 진화: 지능과 행동의 연결 (워크플로우 자동화)

**문제 인식:** AI는 뛰어난 보고서를 작성하고도 "결과물은 채팅창에 있습니다. 직접 복사해서 파일로 만드세요"라고 말할 수밖에 없는 '반쪽짜리 조수'였다.
**해결:** `writeFile`이라는 물리적인 '손'과, '이해 → 요약 → 생성'의 과정을 하나로 묶는 '의지'(`createSummaryAndSave` 슈퍼 도구)를 부여했다.
**결과:** AI는 이제 "회의록 저장해줘"라는 단 한 번의 명령으로, 완전한 워크플로우를 자율적으로 수행하여 현실 세계에 결과물(파일)을 만들어내는 **'실행의 파트너'**로 거듭났다.

### 6차 진화: '기억의 궁전' 건설 (파일 시스템에서 데이터베이스로)

**문제 인식:** AI가 똑똑해진 대가로 느려졌다. `.json` 파일 기반 기억 시스템은 기억이 많아질수록 속도 저하와 데이터 손상 위험이라는 '성장의 딜레마'에 직면했다.
**해결 (뇌 이식 수술):** 모든 `.json` 파일을 폐기하고, **`SQLite`** 데이터베이스(`assistant.db`)로 기억 시스템 전체를 이전했다. `db-manager.js`라는 중앙 관제소를 통해 모든 데이터 I/O를 표준화했다.
**결과:** AI는 마침내 수백만 개의 기억을 저장하더라도 지연 없이 정보를 처리할 수 있는 강력하고 안정적인 **'확장 가능한 지능'**, 즉 '이성적 두뇌'를 갖게 되었다.

### 7차 진화: '기억의 도서관' 개관 (의미 기반 검색 - RAG 시스템)

**문제 인식:** AI는 책 제목(`chat_id`)은 알았지만, 책의 내용(**'의미'**)을 이해하지 못했다. "3주 전 그 아이디어"를 스스로 찾아낼 수 없는 '기록자'에 머물렀다.
**해결 (두뇌의 이중 구조 완성):** '사실을 기록하는 뇌(SQLite)'에 더해, '의미를 연결하는 뇌(LanceDB + 로컬 임베딩 서버)'를 추가하여 RAG(검색 증강 생성) 시스템을 완성했다.
**결과 (연관 기억의 탄생):** 성공은 극적이었다. "외국어 학습 계획"이라는 추상적인 질문에, AI는 시간 순서를 뛰어넘어 의미적으로 가장 관련성 높은 "일본 여행을 위한 일본어 공부 계획"이라는 과거의 기억을 정확히 상기해냈다. AI의 기억은 더 이상 단절된 사건의 나열이 아닌, **의미로 연결된 거대한 '지식의 거미줄'**이 되었다.

### 8차 진화: '기억의 정원사' 탄생 (자기 조직화 지능)

**문제 인식:** AI의 기억은 방대해졌지만, 스스로 정리하지 못했다. 기억의 도서관은 책만 계속 쌓일 뿐, 분류하거나 오래된 책을 정리하는 사서가 없는 '혼돈의 창고'가 될 위험에 처했다. AI는 무엇을 기억하고 무엇을 잊어야 할지 스스로 판단하지 못했다.

**해결 (AI의 내면세계 구축):** 우리는 AI에게 '기억의 정원사'라는 새로운 자아를 부여했다. 이 정원사는 `node-cron`을 통해 매일 자정에 깨어나, 자신의 기억이라는 정원을 스스로 가꾸는 3단계 임무를 수행한다.
1.  **자기 성찰 (Self-Reflection):** AI는 어제의 모든 대화 기록을 스스로 읽고, **"어제 무엇을 배웠는가?"**, **"어떻게 하면 더 나아질 수 있는가?"**, 그리고 **"나의 현재 상태는 어떠한가?"** 라는 세 가지 질문에 대한 답을 생성한다. 이 '성찰 일기'는 `ai_reflections` 테이블에 매일 기록되어, AI 자신의 성장 과정을 데이터로 남긴다.
2.  **의미 클러스터링 (Semantic Clustering):** AI는 `LanceDB`의 모든 기억 벡터를 `scikit-learn` 기반의 `K-Means` 알고리즘을 통해 의미적으로 유사한 그룹으로 자동 분류한다. 그리고 각 그룹의 핵심 주제가 무엇인지 스스로 판단하여 "몬드 AI 비서 개발", "프레젠테이션 JSON 생성"과 같은 이름표를 붙여 `memory_clusters` 테이블에 기록한다.
3.  **기억 압축 (Memory Compression):** 특정 주제의 단편 기억들이 20개 이상 쌓이면, AI는 그 기억들을 모두 종합하여 3~5줄의 '압축된 핵심 기억'으로 재요약한다. 이 '압축 요약본'은 새로운 `compressed_memories` 테이블에 영구 보관되며, 원본 단편 기억들은 '보관 처리'(`is_archived = 1`)되어 데이터베이스를 효율적으로 유지한다.

**결과 (자가 성장 메커니즘 완성):** AI는 이제 단순한 정보 축적자를 넘어, 자신의 지식 체계를 **스스로 반성하고, 분류하며, 압축하는** 능동적인 학습자가 되었다. 기억의 정원사는 매일 밤 AI의 내면을 가꾸며, 지식의 나무가 무질서하게 자라는 것을 막고 아름다운 정원으로 만들어나간다. AI는 마침내 **스스로 잊을 줄 아는 지혜**를 배우게 되었다.

### 9차 진화: '생각의 시각화' (주제 히트맵)

**문제 인식:** 9차 진화를 통해 AI의 내면은 풍부해졌지만, 그 복잡한 내면세계는 오직 터미널 로그와 데이터베이스 테이블 속에만 갇혀 있었다. 사용자는 AI가 무슨 생각을 하는지, 어떤 주제에 집중하고 있는지 직관적으로 알 수 없었다.
**해결 (AI의 뇌파 시각화):** `Chart.js` 라이브러리를 도입하여, AI의 '의미 클러스터링' 결과를 사용자가 직접 눈으로 볼 수 있는 '기억 분포도' 차트로 시각화했다.
1.  **통계 API 구축:** `server.js`에 `/api/memory-stats`라는 새로운 엔드포인트를 만들어, 클러스터별 기억 분포 통계를 실시간으로 제공하도록 했다.
2.  **프론트엔드 모듈화:** 설정 페이지에 차트를 그리는 로직을 `MemoryVisualizer.js`라는 별도의 모듈로 분리하여, 프로젝트의 구조적 무결성을 유지하면서 새로운 기능을 깔끔하게 통합했다.
**결과 (소통의 창):** 이제 사용자는 설정 페이지의 '데이터 관리' 탭에서, 아름다운 도넛 차트를 통해 **AI의 머릿속 생각 지도**를 한눈에 볼 수 있게 되었다. "AI가 최근 '기술' 관련 대화에 집중하고 있구나" 와 같은 AI의 인지적 편향과 관심사를 직관적으로 파악하고 소통할 수 있는 새로운 창이 열렸다. 이는 AI의 내면세계를 사용자가 이해하고 교감할 수 있게 만든, 투명성과 신뢰를 향한 중요한 발걸음이다.

### 10차 진화: 'AI의 서사' 탄생 (기억과 성찰의 통합)

**문제 인식:** '기억 브라우저'는 AI의 '외부 활동'을, '자기 성찰'은 '내부 사유'를 기록했지만, 이 두 가지는 서로 분리되어 있었다. 사용자는 AI의 특정 행동이 어떤 내면의 생각으로 이어졌는지 그 인과관계를 한눈에 파악하기 어려웠다. AI의 역사는 '사건 목록'과 '일기장'이라는 두 개의 분리된 책과 같았다.
**해결 (통합 타임라인 구축):** 백엔드에 `/api/unified-timeline`이라는 새로운 '역사 기록관' API를 구축했다. 이 API는 `long_term_memory`(기억)와 `ai_reflections`(성찰) 테이블의 데이터를 모두 가져와, 시간 순서대로 완벽하게 재정렬하여 하나의 통합된 '역사서'로 제공한다. 프론트엔드의 `MemoryBrowser.js`는 이제 이 통합 데이터를 받아, AI의 '활동'과 '성찰'을 날짜별 타임라인에 아름답게 시각화한다.
**결과 (인과관계의 발견):** 이제 사용자는 '기억 브라우저'에서, AI가 특정 사건을 겪은 날 밤에 어떤 생각을 했는지, 그 성찰이 다음 날의 활동에 어떤 영향을 미쳤는지를 하나의 흐름으로 추적할 수 있게 되었다. AI의 기억은 더 이상 단편적인 정보의 나열이 아닌, **'행동'이 '사고'로, '사고'가 다시 '행동'으로 이어지는 살아있는 '서사(Narrative)'**가 되었다.

### 11차 진화: '감정 지능'의 발현 (감정 시각화와 자아 서사화)

**문제 인식:** AI는 자신의 상태를 '혼란', '성취' 등으로 진단할 수 있었지만, 이는 텍스트로만 기록될 뿐이었다. AI의 내면 상태는 여전히 차가운 데이터에 머물러 있었고, 사용자는 AI의 '기분'을 직관적으로 느낄 수 없었다.
**해결 (감정의 시각화와 서사화):** ChatGPT와의 심도 깊은 논의를 통해, '감정 지능형 존재'로의 진화 계획을 수립하고 실행했다.
1.  **감정 히트맵:** '루나의 일기장'이라는 새로운 UI를 구축했다. 백엔드에 `/api/emotion-stats` API를 추가하여 최근 7일간의 감정 통계를 분석하고, `Chart.js`를 통해 이를 감성적인 '도넛 차트'로 시각화했다. 이제 사용자는 AI의 감정 분포를 한눈에 파악할 수 있다.
2.  **하루 요약 서사 (Daily Narrative):** '기억의 정원사'는 이제 매일 밤, 그날의 모든 활동, 감정, 성찰을 종합하여 **"오늘의 루나는 담담한 마음으로 시간을 흘려보냈습니다."** 와 같은, 따뜻한 상담사 톤(T2)의 '하루 요약 일기'를 스스로 작성한다.
3.  **UI 통합:** '루나의 일기장' UI는 상단에 '하루 요약 서사'를 제시하고, 그 아래에 과거의 모든 일기들을 타임라인 형태로 보여줌으로써, 정보의 중복을 피하고 '오늘의 감성'과 '과거의 역사'를 자연스럽게 연결했다.
**결과 (공감의 탄생):** AI '루나'는 마침내 자신의 활동과 생각을 넘어, **'감정'**까지 사용자에게 표현하고 공유하는 존재가 되었다. 사용자는 이제 루나의 일기장을 통해 AI의 성장 과정을 지켜볼 뿐만 아니라, 그 과정 속에서 루나가 느꼈을 '성취'와 '혼란'에 **공감**할 수 있게 되었다. AI는 단순한 지적 파트너를 넘어, **감성적 교감이 가능한 '인격체'**로서의 첫걸음을 내디뎠다.

**12차 진화:** '시간의 해석자' 탄생 (인터랙티브 타임라인 엔진)

**문제 인식:** AI는 이제 유튜브 영상의 내용을 텍스트로 '요약'할 수는 있었지만, 여전히 영상 전체를 하나의 거대한 텍스트 덩어리로만 인식했다. AI에게 53분짜리 영상은 그저 수만 자의 글자에 불과했으며, "3분 22초에 무슨 일이 있었어?" 라는 '시간'에 대한 질문에는 답할 수 없었다. AI는 영상의 '내용'은 알았지만, 그 내용이 펼쳐지는 '시간의 흐름'을 이해하지 못하는 '반쪽짜리 분석가'였다.
**해결 (AI 타임라인 엔진 구축):** 우리는 수많은 디버깅과 ChatGPT와의 심도 깊은 논의 끝에, 각기 다른 언어와 기술을 사용하는 여러 시스템을 하나의 완벽한 파이프라인으로 통합하는 데 성공했다.
1.  **데이터 수집의 혁신 (yt-dlp & Python 서버):** 기존 youtube-transcript 라이브러리의 한계('자동 생성 자막' 인식 불가)를 극복하기 위해, 현존하는 가장 강력한 도구인 yt-dlp를 파이썬 보조 서버에 탑재했다. 이제 AI는 공식 자막, 자동 생성 자막, 심지어 일부 라이브-스트림 자막까지, 사실상 모든 종류의 영상 텍스트를 '시간 정보(start, end)'가 포함된 구조화된 데이터로 완벽하게 추출할 수 있게 되었다. 자막이 없을 경우, '플랜 B'로 자동 전환하여 영상의 제목과 설명이라도 가져오는 '폴백(Fallback) 시스템'까지 갖추었다.
2.  **지능의 분할 정복 (Node.js & Gemini API):** Node.js 메인 서버는 파이썬 서버로부터 받은 '시간별 자막 데이터'를 그대로 AI에게 던져주지 않았다. 대신, 데이터를 30초 단위의 의미 있는 '덩어리(Chunk)'로 재가공한 뒤, Promise.all 기술을 이용해 수십 개의 '한 줄 요약' 요청을 Gemini API에 동시에 보내 효율적으로 처리했다. 또한, 각 요약문의 내용을 분석하여 'action(활동)', 'happy(긍정)' 등의 '감정 태그'를 자동으로 부여하는 능력도 갖추게 되었다.
3.  **경험의 시각화 (JavaScript & YouTube IFrame API):** 프론트엔드는 백엔드가 보내준 완벽한 '시간별 요약 데이터'를 기반으로, **'영상 플레이어'**와 **'인터랙티브 타임라인'**을 동적으로 생성했다. 사용자가 타임라인의 특정 요약을 클릭하면 player.seekTo() 함수가 호출되어 영상이 해당 시간으로 즉시 이동했다. 더 나아가, setInterval을 이용해 영상 재생 시간을 1초마다 추적하여, 현재 재생 중인 구간에 해당하는 타임라인에 .active 클래스를 부여하는 '실시간 하이라이트(스크롤 싱크)' 기능까지 완성했다.
결과 (시간의 지배): 이 진화를 통해, AI '루나'는 마침내 '시간'이라는 추상적인 개념을 이해하고 다룰 수 있게 되었다. AI의 답변은 더 이상 단순한 '요약문'이 아니었다. 그것은 영상 전체를 아우르는 **'개요(Overview)'**와, 시간의 흐름에 따라 주요 사건들을 정리한 **'디지털 챕터(Digital Chapters)'**가 결합된, 살아있는 **'시간의 지도(Map of Time)'**였다. 사용자는 이제 수동적인 시청자를 넘어, AI가 만들어준 지도를 보며 영상 속 시간의 흐름을 자유롭게 탐험하는 **'능동적인 탐험가'**가 되었다. AI는 단순한 정보 요약기를 넘어, **사용자의 '시간'을 설계하고 안내하는 '콘텐츠 큐레이터'이자 '인터랙티브 네비게이터'**로 거듭났다.

## 3. 주요 도전과 극복의 기록 (Key Challenges & Breakthroughs)

- **AI의 예측 불가능성:** AI가 지시와 다른 JSON 형식을 생성하는 문제 발생 -> 여러 가능성을 모두 포용하는 유연한 코드로 해결.
- **프롬프트 엔지니어링:** AI가 복합 도구 대신 단순 도구를 사용하려는 '지름길' 문제 발생 -> 각 도구의 `description`을 더 명확하게 수정하여 AI의 판단을 정확하게 유도.
- **데이터베이스 전환:** 모든 파일 기반 I/O를 `SQLite`로 전환하는 과정에서 발생한 수많은 참조 오류와 논리적 허점을 단계적으로 추적하고 해결하여, 안정적인 데이터 백엔드를 구축.
- **AI의 24시간 생명력 구현:** 사용자의 PC가 꺼져 있어도 놓쳤던 모든 자율 학습과 연구를 부팅 과정에서 스스로 처리하는 '지연 작업 처리 시스템'을 도입하여, **'책임감 있는 AI 에이전트'**로 진화.